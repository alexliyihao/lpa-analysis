{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write(path, content):\n",
    "    \"\"\"\n",
    "    a wrapper function for with-open-write, just for good code looking...\n",
    "    \"\"\"\n",
    "    with open(path, \"w+\") as f:\n",
    "        read_data = f.read()\n",
    "        f.write(content)\n",
    "        \n",
    "def append(path, content):\n",
    "    \"\"\"\n",
    "    a wrapper function for with-open-append, just for good code looking...\n",
    "    \"\"\"\n",
    "    with open(path, \"a+\") as f:\n",
    "        read_data = f.read()\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ID(SampleID):\n",
    "    \"\"\"\n",
    "    Helper function clean Sample ID to pure digits\n",
    "    \n",
    "    Args:\n",
    "        SampleID: String, in \"washei*****.BQSR.recaled.bam\" format, where * stand for numbers\n",
    "    Return:\n",
    "        String, the number characters in the middle\n",
    "    \"\"\"\n",
    "    return re.findall(r\"[0-9]+\", SampleID)[0]\n",
    "\n",
    "def get_file(input_path, bam_name):\n",
    "    '''\n",
    "    given a bam name, read the variantsAnnotate.txt inside the output of Coassin pipeline\n",
    "    Args:\n",
    "        input_path, the input path with all the bam output inside\n",
    "        bam_name: str, the name of the original bam file\n",
    "    Return:\n",
    "        pandas.DataFrame instance, the variantsAnnotate.txt file read\n",
    "    '''\n",
    "    return pd.read_csv(f'{input_path}/{bam_name}/variantsAnnotate/variantsAnnotate.txt', delimiter = \"\\t\")\n",
    "\n",
    "def data_cleaning(df):\n",
    "    \"\"\"\n",
    "    existing data cleaning procedure\n",
    "    Args:\n",
    "        df: pandas.DafaFrame instance, the dataframe\n",
    "    Returns:\n",
    "        df: pandas.DafaFrame instance, the dataframe with cleaned ID\n",
    "    \"\"\"\n",
    "    df.SampleID = df.SampleID.apply(extract_ID)\n",
    "    return df\n",
    "\n",
    "def data_load_wrapper(input_path, bam_list):\n",
    "    \"\"\"\n",
    "    The wrapper for a loading files\n",
    "    Args:\n",
    "        path_list: list[str], the path hold all Coassin pipeline output\n",
    "    Return:\n",
    "        df: pandas.DafaFrame instance, all the data should be loaded\n",
    "    \"\"\"\n",
    "    return pd.concat([\n",
    "        data_cleaning(\n",
    "            get_file(input_path = input_path,\n",
    "                     bam_name = bam_name)\n",
    "        ) \n",
    "        for bam_name in bam_list\n",
    "    ], \n",
    "        axis = \"index\", \n",
    "        ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_meta(file_format = \"VCFv4.2\", \n",
    "                  file_date = 'today', \n",
    "                  source = \"lpa-analysis\",\n",
    "                  reference = \"https://raw.githubusercontent.com/seppinho/mutation-server/76e865ece25cf792d1534b0288b2c28bc1b3d013/test-data/dna/lpa-sample/reference/kiv2_6.fasta\"\n",
    "                 ):\n",
    "    meta_information = \\\n",
    "f'''##fileformat={file_format}\n",
    "##fileDate={datetime.datetime.now().strftime(\"%Y%m%d\") if file_date==\"today\" else file_date}\n",
    "##source={source}\n",
    "##reference={reference}\n",
    "##phasing=partial \n",
    "##FORMAT=<ID=GT,Number=1,Type=String,Description=\"Genotype\"> \n",
    "##FORMAT=<ID=VL,Number=1,Type=Float,Description=\"Variant Level\"> \n",
    "##FORMAT=<ID=TC,Number=1,Type=Integer,Description=\"Total Coverage\"> \n",
    "##FORMAT=<ID=TB,Number=1,Type=String,Description=\"TypeB\"> \n",
    "'''\n",
    "    return meta_information\n",
    "\n",
    "def generate_header(sample_id_sorted_list):\n",
    "    return \"\\t\".join(['#CHROM', 'POS', 'ID', 'REF', 'ALT', 'QUAL', 'FILTER', 'FORMAT'] + sample_id_sorted_list)+\"\\n\"\n",
    "\n",
    "def new_index(l, i):\n",
    "    \"\"\"\n",
    "    a exception solver for list with exceed index\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return l[i]\n",
    "    except:\n",
    "        return \"0/0:::\"\n",
    "    \n",
    "def generate_line(pos, REF, ALT, sample_literal):\n",
    "    CHROM = \"6\"\n",
    "    POS = str(pos)\n",
    "    ID = '.'\n",
    "    QUAL = '.'\n",
    "    FILTER = '.'\n",
    "    FORMAT = \"GT:VL:TC:TB\"\n",
    "    repeat = max([len(l) for l in sample_literal])\n",
    "    # when there is no multiple output in one position for one sample\n",
    "    if repeat == 1:\n",
    "        return \"\\t\".join([CHROM, POS, ID, REF, ALT, QUAL, FILTER, FORMAT]+[i[0] for i in sample_literal])+\"\\n\"\n",
    "    #when there are multiple output in one position for one sample\n",
    "    else:\n",
    "        return \"\\n\".join(\n",
    "            [\"\\t\".join(\n",
    "                [CHROM, POS, ID, REF, ALT, QUAL, FILTER, FORMAT]+\n",
    "                [new_index(l,i) for l in sample_literal])\n",
    "             for i in range(repeat)\n",
    "            ]\n",
    "        )+\"\\n\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_data(df):\n",
    "    \"\"\"\n",
    "    Extract the positional-specific data, df is expected to be the return of\n",
    "    pandas.DataFrameGroupBy.get_group(), which is grouped by the position(\"Pos\")\n",
    "    \"\"\"\n",
    "    # reference is unique for each position\n",
    "    REF = df.Ref.unique()[0]\n",
    "    # This list keeps the order of alternatives\n",
    "    alt_list = list(df.Variant.unique())\n",
    "    # the output format of alternatives\n",
    "    ALT = ','.join(alt_list)\n",
    "    # Total Coverage is unique for each position\n",
    "    TC = df['Coverage-Total'].unique()[0]\n",
    "    return REF, ALT, TC, alt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_genotype_literal(row, ref, alt_list):\n",
    "    '''\n",
    "    generate the genotype information literal\n",
    "    args:\n",
    "        row: pandas dataframe individual line, a variant record\n",
    "        ref: str, the reference position\n",
    "        alt_list: all the alternatives possible\n",
    "    return:\n",
    "        a genotype output follows VCFv4.2 1.4.2 genotype field requirement given the order from alt_list\n",
    "    '''\n",
    "    gt_list = [ref]+alt_list\n",
    "    gt_dict = {v: k for k, v in enumerate(gt_list)}\n",
    "    major, minor = row['Major/Minor'].split('/')\n",
    "    return f\"{gt_dict[major]}/{gt_dict[minor]}\"\n",
    "\n",
    "def _get_genotype(df, ref, alt_list):\n",
    "    \"\"\"\n",
    "    a wrapper apply _genotype to each individual line of df\n",
    "    args:\n",
    "        df: pandas.DataFrame object, a list of variant record\n",
    "        ref: str, the reference position\n",
    "        alt_list: all the alternatives possible\n",
    "    return:\n",
    "        list[str]: genotype output follows VCFv4.2 1.4.2 genotype field requirement given the order from alt_list\n",
    "    \"\"\"\n",
    "    # apply _get_genotype_literal to all rows\n",
    "    return dict(\n",
    "        df.apply(\n",
    "            func = (lambda x: _get_genotype_literal(row = x, ref = ref, alt_list = alt_list)),\n",
    "            axis=1)\n",
    "    )\n",
    "\n",
    "def _extract_data(df, ref, alt_list, TC):\n",
    "    \"\"\"\n",
    "    generate necessary data format from a pandas.DataFrame object,\n",
    "    This DataFrame object is designed to be generated from \n",
    "    a get_group method from a pandas.DataFrameGroupBy object\n",
    "    \"\"\"\n",
    "\n",
    "    # extract the genotype notation string\n",
    "    GT = _get_genotype(df = df, ref = ref, alt_list = alt_list)\n",
    "    # get the variant level information\n",
    "    VL = dict(df['Variant-Level'])\n",
    "    # get the typeB information\n",
    "    TB = dict(df[\"TypeB\"])\n",
    "    # build the notation literals\n",
    "    notation = [f\"{GT[index]}:{VL[index]:.4f}:{TC}:{TB[index]}\" for index in df.index]\n",
    "\n",
    "    return notation\n",
    "\n",
    "def get_sample_data(df_group_by_id, SampleID, ref, alt_list, TC):\n",
    "    \"\"\"\n",
    "    a wrapper for _extract_data, which dealt with the keyError and generate a \"0/0:::\" literal\n",
    "    return:\n",
    "        list[str]: a list with genotype literals inside\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = df_group_by_id.get_group(SampleID)\n",
    "    except KeyError:\n",
    "        return [\"0/0:::\"]\n",
    "    # if there is any duplicate use comma for now\n",
    "    return _extract_data(df = df, ref = ref, alt_list = alt_list, TC = TC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_VCF(input_path = \"data\", output_path = \"output.vcf\", bam_list = [\"*\"]):\n",
    "    \"\"\"\n",
    "    Wrapper function for a complete process taking Coassin pipeline output as input\n",
    "    and output VCF file at output_path\n",
    "    \n",
    "    args:\n",
    "        input_path: str, the path of input folder with all the Coassin Pipeline output inside the folder\n",
    "        bam_list: list[str], if specified it will select the file in the list\n",
    "        output_path: str, the path of output path\n",
    "        \n",
    "    output:\n",
    "        a file at <output_path> following VCFv4.2 requirement\n",
    "    \"\"\"\n",
    "    \n",
    "    if bam_list == [\"*\"]:\n",
    "        input_list = os.listdir(input_path)\n",
    "        \n",
    "    # Load the complete dataset into pandas\n",
    "    df = data_load_wrapper(input_path, bam_list)\n",
    "    \n",
    "    # Write the output path\n",
    "    write(path = output_path, content = generate_meta())\n",
    "    \n",
    "    # get all the SampleID(this ID is str, but it's still sorted numerically in python)\n",
    "    sample_id_sorted_list = list([i for i in df.SampleID.sort_values().drop_duplicates()])\n",
    "    \n",
    "    # append the header\n",
    "    append(path = output_path, \n",
    "           content = generate_header(sample_id_sorted_list = sample_id_sorted_list))\n",
    "    \n",
    "    # get all the positions\n",
    "    pos_sorted_list = list([i for i in df.Pos.sort_values().drop_duplicates()])\n",
    "    \n",
    "    # group the results by Positions(\"Pos\"), sort = False to improve efficiency\n",
    "    df_group_by_pos = df.groupby(['Pos'], sort = False)\n",
    "    \n",
    "    # iterate over the positions, to save the memory the result will be output line by line, i.e. by position\n",
    "    for pos in pos_sorted_list:\n",
    "        \n",
    "        # get all the records on position <pos>\n",
    "        df_group_pos = df_group_by_pos.get_group(pos)\n",
    "        \n",
    "        # get the positional data, which will not be affected or can be generated by individual Samples\n",
    "        REF, ALT, TC, alt_list = get_positional_data(df_group_pos)\n",
    "        \n",
    "        # group the records by sample id (\"SampleID\"), sort = False to improve efficiency\n",
    "        df_group_by_id = df_group_pos.groupby(\"SampleID\", sort = False)\n",
    "        \n",
    "        # get data from each individual sample\n",
    "        sample_literal = [get_sample_data(\n",
    "            df_group_by_id = df_group_by_id, \n",
    "            SampleID = sample_id, \n",
    "            ref = REF, \n",
    "            alt_list = alt_list, \n",
    "            TC = TC\n",
    "        ) for sample_id in sample_id_sorted_list]\n",
    "        # complete the row with sample data\n",
    "        append(path = output_path, \n",
    "               content = generate_line(pos = pos,\n",
    "                                       REF = REF,\n",
    "                                       ALT = ALT,\n",
    "                                       sample_literal = sample_literal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_VCF(input_path = \"data\", \n",
    "          output_path = \"test.vcf\", \n",
    "          bam_list = ['washei49194.BQSR.recaled.bam',\n",
    "                      'washei62376.BQSR.recaled.bam',\n",
    "                      'washei45948.BQSR.recaled.bam',\n",
    "                      'washei55083.BQSR.recaled.bam',\n",
    "                      'washei50433.BQSR.recaled.bam']\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
